{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation using Seq2Seq Model with English-German dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.nn import LayerNorm\n",
    "from itertools import chain\n",
    "import numpy as np \n",
    "import spacy\n",
    "import random \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two young, White males are outside near many b...</td>\n",
       "      <td>Zwei junge weiße Männer sind im Freien in der ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Several men in hard hats are operating a giant...</td>\n",
       "      <td>Mehrere Männer mit Schutzhelmen bedienen ein A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A little girl climbing into a wooden playhouse.</td>\n",
       "      <td>Ein kleines Mädchen klettert in ein Spielhaus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man in a blue shirt is standing on a ladder ...</td>\n",
       "      <td>Ein Mann in einem blauen Hemd steht auf einer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Two men are at the stove preparing food.</td>\n",
       "      <td>Zwei Männer stehen am Herd und bereiten Essen zu.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  Two young, White males are outside near many b...   \n",
       "1  Several men in hard hats are operating a giant...   \n",
       "2    A little girl climbing into a wooden playhouse.   \n",
       "3  A man in a blue shirt is standing on a ladder ...   \n",
       "4           Two men are at the stove preparing food.   \n",
       "\n",
       "                                              german  \n",
       "0  Zwei junge weiße Männer sind im Freien in der ...  \n",
       "1  Mehrere Männer mit Schutzhelmen bedienen ein A...  \n",
       "2  Ein kleines Mädchen klettert in ein Spielhaus ...  \n",
       "3  Ein Mann in einem blauen Hemd steht auf einer ...  \n",
       "4  Zwei Männer stehen am Herd und bereiten Essen zu.  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"D:/College/5th Semester/NLP/Assignmnet 1/translation_train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>german</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A man in an orange hat starring at something.</td>\n",
       "      <td>Ein Mann mit einem orangefarbenen Hut, der etw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Boston Terrier is running on lush green gras...</td>\n",
       "      <td>Ein Boston Terrier läuft über saftig-grünes Gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A girl in karate uniform breaking a stick with...</td>\n",
       "      <td>Ein Mädchen in einem Karateanzug bricht einen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Five people wearing winter jackets and helmets...</td>\n",
       "      <td>Fünf Leute in Winterjacken und mit Helmen steh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>People are fixing the roof of a house.</td>\n",
       "      <td>Leute Reparieren das Dach eines Hauses.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0      A man in an orange hat starring at something.   \n",
       "1  A Boston Terrier is running on lush green gras...   \n",
       "2  A girl in karate uniform breaking a stick with...   \n",
       "3  Five people wearing winter jackets and helmets...   \n",
       "4             People are fixing the roof of a house.   \n",
       "\n",
       "                                              german  \n",
       "0  Ein Mann mit einem orangefarbenen Hut, der etw...  \n",
       "1  Ein Boston Terrier läuft über saftig-grünes Gr...  \n",
       "2  Ein Mädchen in einem Karateanzug bricht einen ...  \n",
       "3  Fünf Leute in Winterjacken und mit Helmen steh...  \n",
       "4            Leute Reparieren das Dach eines Hauses.  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test=pd.read_csv(\"D:/College/5th Semester/NLP/Assignmnet 1/translation_test.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "english=df['english'].tolist()\n",
    "german=df['german'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN=20 # For computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "def create_tokenizer(text_corpus, vocab_limit=10000):\n",
    "    # Create a counter to count the word frequencies in the corpus\n",
    "    word_counter = Counter(chain.from_iterable(sentence.split() for sentence in text_corpus))\n",
    "    \n",
    "    # Build a vocabulary with a limit\n",
    "    vocabulary = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"] + [word for word, _ in word_counter.most_common(vocab_limit - 4)]\n",
    "    \n",
    "    # Build mappings for word-to-index and index-to-word\n",
    "    word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "    \n",
    "    # Tokenize function will return a list of integers (the word indices)\n",
    "    def tokenize(sentence):\n",
    "        # Use get() to prevent key errors (if a word is not in the vocabulary, it maps to <unk>)\n",
    "        return [word_to_index.get(word, word_to_index[\"<unk>\"]) for word in sentence.split()]\n",
    "    \n",
    "    return tokenize, word_to_index, index_to_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MTDataset(Dataset):\n",
    "    def __init__(self, input_texts, output_texts, input_tokenizer, output_tokenizer, sequence_limit):\n",
    "        # Use list comprehension to process the input and output sequences\n",
    "        self.input_sequences = [input_tokenizer(text)[:sequence_limit] for text in input_texts]\n",
    "        self.output_sequences = [output_tokenizer(text)[:sequence_limit] for text in output_texts]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_sequence = torch.tensor(self.input_sequences[index], dtype=torch.long)\n",
    "        output_sequence = torch.tensor(self.output_sequences[index], dtype=torch.long)\n",
    "        return input_sequence, output_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def batch_collate_fn(batch):\n",
    "    input_batch, output_batch = zip(*batch) \n",
    "    # Pad the input and output sequences\n",
    "    input_padded = pad_sequence(input_batch, batch_first=True, padding_value=0)\n",
    "    output_padded = pad_sequence(output_batch, batch_first=True, padding_value=0)\n",
    "    return input_padded, output_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "training_input,validation_input,training_output,validation_output=train_test_split(english,german,test_size=0.2,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_input=df_test['english'].tolist()\n",
    "testing_output=df_test['german'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer,eng_word2idx,eng_idx2word=create_tokenizer(training_input)\n",
    "ger_tokenizer,ger_word2idx,gr_idx2word=create_tokenizer(training_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=MTDataset(training_input,training_output,eng_tokenizer,ger_tokenizer,MAX_SEQ_LEN)\n",
    "val_dataset=MTDataset(validation_input,validation_output,eng_tokenizer,ger_tokenizer,MAX_SEQ_LEN)\n",
    "test_dataset=MTDataset(testing_input,testing_output,eng_tokenizer,ger_tokenizer,MAX_SEQ_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,collate_fn=batch_collate_fn)\n",
    "val_loader=DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=False,collate_fn=batch_collate_fn)\n",
    "test_loader=DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=False,collate_fn=batch_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "embed_dim = 256  \n",
    "vocab = len(eng_word2idx) \n",
    "\n",
    "# Training the Word2Vec model\n",
    "model_2 = Word2Vec(sentences=[sentence.split() for sentence in df['english'].tolist()], \n",
    "                   vector_size=embed_dim, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Initialize embedding matrix with random values\n",
    "embedding_matrix = np.random.uniform(-0.05, 0.05, (vocab, embed_dim))\n",
    "\n",
    "# Update the embedding matrix with pre-trained Word2Vec vectors\n",
    "for word, idx in eng_word2idx.items():\n",
    "    if word in model_2.wv:\n",
    "        embedding_matrix[idx] = model_2.wv[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 256)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEQ2SEQ Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers,embedding_matrix):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding.weight.data.copy_(torch.tensor(embedding_matrix,dtype=torch.float32)) \n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers,embedding_matrix):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding.weight.data.copy_(torch.tensor(embedding_matrix,dtype=torch.float32)) \n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = x.unsqueeze(1)  # Add time-step dimension\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        predictions = self.fc(outputs.squeeze(1))\n",
    "        return predictions, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Early_Stopping:\n",
    "    def __init__(self, patience=3, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss  \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()  \n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()  \n",
    "            self.counter = 0\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        model.load_state_dict(self.best_model_state) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.num_layers = decoder.lstm.num_layers\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        trg_vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(src.device)\n",
    "\n",
    "        # Encoder forward pass\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # Use the last forward states from the encoder\n",
    "        hidden = hidden[-2].unsqueeze(0).repeat(self.num_layers, 1, 1)  # Repeat for num_layers\n",
    "        cell = cell[-2].unsqueeze(0).repeat(self.num_layers, 1, 1)      # Repeat for num_layers\n",
    "\n",
    "        # Decoder forward pass\n",
    "        x = trg[:, 0]  # Start token (<sos>)\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            x = trg[:, t] if teacher_force else output.argmax(1)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=len(eng_word2idx)\n",
    "output_dim=len(ger_word2idx)\n",
    "embed_dim=256\n",
    "hidden_dim=512\n",
    "num_layers=2\n",
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=Encoder(input_dim,embed_dim,hidden_dim,num_layers,embedding_matrix)\n",
    "decoder=Decoder(output_dim,embed_dim,hidden_dim,num_layers,embedding_matrix)\n",
    "model=Seq2Seq(encoder,decoder).to(device)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion=nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 epoch loss: 6.0927\n",
      "Epoch 2 epoch loss: 5.6066\n",
      "Epoch 3 epoch loss: 5.4055\n",
      "Epoch 4 epoch loss: 5.1414\n",
      "Epoch 5 epoch loss: 4.8698\n",
      "Epoch 6 epoch loss: 4.6085\n",
      "Epoch 7 epoch loss: 4.3740\n",
      "Epoch 8 epoch loss: 4.1734\n",
      "Epoch 9 epoch loss: 3.9913\n",
      "Epoch 10 epoch loss: 3.8138\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, trg in train_loader:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        avg_loss=epoch_loss/len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} epoch loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\College\\5th Semester\\NLP\\Assignmnet 1\\myenv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "d:\\College\\5th Semester\\NLP\\Assignmnet 1\\myenv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "d:\\College\\5th Semester\\NLP\\Assignmnet 1\\myenv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average sentence-level BLEU score is 0.0150\n",
      "The corpus-level BLEU score is 0.0395\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "model.eval()  \n",
    "predicted_sentences, reference_sentences = [], []  # Lists to store sentences\n",
    "bleu_results = []\n",
    "\n",
    "with torch.no_grad():  \n",
    "    for src, trg in test_loader:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        model_output = model(src, trg, 0) \n",
    "        predicted_indices = model_output.argmax(2)  \n",
    "        for i in range(len(predicted_indices)): \n",
    "            predicted_words = [gr_idx2word.get(idx.item(), '<unk>') for idx in predicted_indices[i]]\n",
    "            reference_words = [gr_idx2word.get(idx.item(), '<unk>') for idx in trg[i]]\n",
    "            predicted_words = [word for word in predicted_words if word not in (\"<pad>\", \"<sos>\", \"<eos>\")]\n",
    "            reference_words = [word for word in reference_words if word not in (\"<pad>\", \"<sos>\", \"<eos>\")]\n",
    "            predicted_sentences.append(predicted_words)\n",
    "            reference_sentences.append([reference_words])  \n",
    "            bleu_results.append(sentence_bleu([reference_words], predicted_words))\n",
    "avg_bleu_score = sum(bleu_results) / len(bleu_results)\n",
    "print(f\"The average sentence-level BLEU score is {avg_bleu_score:.4f}\")\n",
    "corpus_bleu_score = corpus_bleu(reference_sentences, predicted_sentences)\n",
    "print(f\"The corpus-level BLEU score is {corpus_bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's use the GPT-2 transformer Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to prepare dataset\n",
    "def prepare_gpt2_dataset(df, sep_token=\"<|SEP|>\"):\n",
    "    formatted_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        src = row[\"english\"]\n",
    "        trg = row[\"german\"]\n",
    "        formatted_data.append(f\"{src} {sep_token} {trg}\")\n",
    "    return formatted_data\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer_gpt = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model_gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = {\"sep_token\": \"<|SEP|>\", \"pad_token\": \"<|PAD|>\"}\n",
    "tokenizer_gpt.add_special_tokens(special_tokens)\n",
    "tokenizer_gpt.pad_token = \"<|PAD|>\"\n",
    "model_gpt.resize_token_embeddings(len(tokenizer_gpt))  # Update model embeddings\n",
    "model_gpt.config.pad_token_id = tokenizer_gpt.pad_token_id\n",
    "\n",
    "\n",
    "# Prepare dataset\n",
    "gpt2_texts = prepare_gpt2_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class GPT2Dataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "# Create dataset and dataloader\n",
    "max_length = 128\n",
    "batch_size = 16\n",
    "dataset = GPT2Dataset(gpt2_texts, tokenizer_gpt, max_length=max_length)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\College\\5th Semester\\NLP\\Assignmnet 1\\myenv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0662\n",
      "Epoch 2, Loss: 0.6081\n",
      "Epoch 3, Loss: 0.5361\n"
     ]
    }
   ],
   "source": [
    "# Move model to device and define optimizer\n",
    "model_gpt.to(device)\n",
    "optimizer = AdamW(model_gpt.parameters(), lr=5e-5)\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model_gpt.train()\n",
    "    epoch_loss = 0   \n",
    "    for input_ids, attention_mask in train_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model_gpt(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Print epoch loss\n",
    "    model_gpt.save_pretrained(\"gpt2_finetuned_checkpoint\")\n",
    "    tokenizer_gpt.save_pretrained(\"gpt2_finetuned_checkpoint\")\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2texs_test=prepare_gpt2_dataset(df_test)\n",
    "dataset_test=GPT2Dataset(gpt2texs_test,tokenizer_gpt,max_length=max_length)\n",
    "test_loader=DataLoader(dataset_test,batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "src[src >= tokenizer_gpt.vocab_size] = tokenizer_gpt.pad_token_id\n",
    "src[src < 0] = tokenizer_gpt.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1: 0.2352\n",
      "Average ROUGE-2: 0.1188\n",
      "Average ROUGE-L: 0.1987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.23523840144164126, 0.11882599355287717, 0.198748926828044)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Function to generate text using the trained model\n",
    "def generate_translation(model, tokenizer, src_sentence, max_length=128):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    inputs = tokenizer.encode(src_sentence, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    \n",
    "    # Create attention mask for padding (1 for non-padding, 0 for padding)\n",
    "    attention_mask = (inputs != tokenizer.pad_token_id).long()\n",
    "    \n",
    "    # Ensure that pad_token_id is properly set during generation\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs.to(device),\n",
    "        attention_mask=attention_mask.to(device),\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id  # Explicitly set the pad_token_id\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Function to evaluate ROUGE scores\n",
    "def evaluate_with_rouge(model, tokenizer, test_df, max_length=128):\n",
    "    model.eval()\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    \n",
    "    total_rouge1 = 0\n",
    "    total_rouge2 = 0\n",
    "    total_rougeL = 0\n",
    "    num_samples = len(test_df)\n",
    "    \n",
    "    # Generate predictions and compare to target\n",
    "    with torch.no_grad():\n",
    "        for i, row in test_df.iterrows():\n",
    "            src_sentence = row[\"english\"]\n",
    "            trg_sentence = row[\"german\"]\n",
    "            \n",
    "            # Generate predicted translation\n",
    "            pred_translation = generate_translation(model, tokenizer, src_sentence, max_length)\n",
    "            \n",
    "            # Compute ROUGE scores between the predicted and target translation\n",
    "            scores = scorer.score(trg_sentence, pred_translation)\n",
    "            \n",
    "            total_rouge1 += scores[\"rouge1\"].fmeasure\n",
    "            total_rouge2 += scores[\"rouge2\"].fmeasure\n",
    "            total_rougeL += scores[\"rougeL\"].fmeasure\n",
    "\n",
    "    # Calculate average ROUGE scores\n",
    "    avg_rouge1 = total_rouge1 / num_samples\n",
    "    avg_rouge2 = total_rouge2 / num_samples\n",
    "    avg_rougeL = total_rougeL / num_samples\n",
    "    \n",
    "    print(f\"Average ROUGE-1: {avg_rouge1:.4f}\")\n",
    "    print(f\"Average ROUGE-2: {avg_rouge2:.4f}\")\n",
    "    print(f\"Average ROUGE-L: {avg_rougeL:.4f}\")\n",
    "    \n",
    "    return avg_rouge1, avg_rouge2, avg_rougeL\n",
    "\n",
    "# Evaluate using ROUGE on test data\n",
    "evaluate_with_rouge(model_gpt, tokenizer_gpt, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, tokenizer, input_sentence, max_length=128, max_new_tokens=50):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        input_sentence, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=max_length\n",
    "    ).to(device)\n",
    "    \n",
    "    # Move tensors to the device\n",
    "    inputs = {key: value for key, value in inputs.items()}\n",
    "    model.to(device)\n",
    "    # Generate output\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'].to(device), \n",
    "        attention_mask=inputs['attention_mask'], \n",
    "        max_length=max_length + max_new_tokens,  # Extend max_length\n",
    "        max_new_tokens=max_new_tokens,          # Limit number of new tokens\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the result\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hello my name is Habiba\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=50) and `max_length`(=178) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello my name is Habiba Ein Hände ist Habiba.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_translated=translate_sentence(model_gpt,tokenizer_gpt,sentence)\n",
    "sentence_translated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
