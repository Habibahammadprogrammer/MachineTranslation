{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Transformer Model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:50.364731Z",
     "iopub.status.busy": "2024-12-22T13:22:50.364340Z",
     "iopub.status.idle": "2024-12-22T13:22:54.746479Z",
     "shell.execute_reply": "2024-12-22T13:22:54.745838Z",
     "shell.execute_reply.started": "2024-12-22T13:22:50.364700Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:54.748848Z",
     "iopub.status.busy": "2024-12-22T13:22:54.748120Z",
     "iopub.status.idle": "2024-12-22T13:22:54.953358Z",
     "shell.execute_reply": "2024-12-22T13:22:54.952687Z",
     "shell.execute_reply.started": "2024-12-22T13:22:54.748794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/College/5th Semester/NLP\\Assignmnet 1/translation_train.csv')\n",
    "df_test=pd.read_csv('D:/College/5th Semester/NLP/Assignmnet 1/translation_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:54.955275Z",
     "iopub.status.busy": "2024-12-22T13:22:54.954864Z",
     "iopub.status.idle": "2024-12-22T13:22:54.988630Z",
     "shell.execute_reply": "2024-12-22T13:22:54.987841Z",
     "shell.execute_reply.started": "2024-12-22T13:22:54.955246Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29000, 2)\n",
      "english    0\n",
      "german     0\n",
      "dtype: int64\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.isnull().sum())\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "english    0\n",
      "german     0\n",
      "dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df_test.shape)\n",
    "print(df_test.isnull().sum())\n",
    "print(df_test.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:55.016772Z",
     "iopub.status.busy": "2024-12-22T13:22:55.016538Z",
     "iopub.status.idle": "2024-12-22T13:22:55.038310Z",
     "shell.execute_reply": "2024-12-22T13:22:55.037542Z",
     "shell.execute_reply.started": "2024-12-22T13:22:55.016753Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english    0\n",
      "german     0\n",
      "dtype: int64\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:55.039522Z",
     "iopub.status.busy": "2024-12-22T13:22:55.039280Z",
     "iopub.status.idle": "2024-12-22T13:22:55.059378Z",
     "shell.execute_reply": "2024-12-22T13:22:55.058796Z",
     "shell.execute_reply.started": "2024-12-22T13:22:55.039502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:55.080932Z",
     "iopub.status.busy": "2024-12-22T13:22:55.080685Z",
     "iopub.status.idle": "2024-12-22T13:22:55.086367Z",
     "shell.execute_reply": "2024-12-22T13:22:55.085415Z",
     "shell.execute_reply.started": "2024-12-22T13:22:55.080914Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.\n",
      "Several men in hard hats are operating a giant pulley system.\n",
      "Ein kleines Mädchen klettert in ein Spielhaus aus Holz.\n",
      "A little girl climbing into a wooden playhouse.\n",
      "Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.\n",
      "A man in a blue shirt is standing on a ladder cleaning a window.\n",
      "Zwei Männer stehen am Herd und bereiten Essen zu.\n",
      "Two men are at the stove preparing food.\n",
      "Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.\n",
      "A man in green holds a guitar while the other man observes his shirt.\n",
      "Ein Mann lächelt einen ausgestopften Löwen an.\n",
      "A man is smiling at a stuffed lion\n",
      "Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt.\n",
      "A trendy girl talking on her cellphone while gliding slowly down the street.\n",
      "Eine Frau mit einer großen Geldbörse geht an einem Tor vorbei.\n",
      "A woman with a large purse is walking by a gate.\n",
      "Jungen tanzen mitten in der Nacht auf Pfosten.\n",
      "Boys dancing on poles in the middle of the night.\n",
      "Eine Ballettklasse mit fünf Mädchen, die nacheinander springen.\n",
      "A ballet class of five girls jumping in sequence.\n",
      "Vier Typen, von denen drei Hüte tragen und einer nicht, springen oben in einem Treppenhaus.\n",
      "Four guys three wearing hats one not are jumping at the top of a staircase.\n",
      "Ein schwarzer Hund und ein gefleckter Hund kämpfen.\n",
      "A black dog and a spotted dog are fighting\n",
      "Ein Mann in einer neongrünen und orangefarbenen Uniform fährt auf einem grünen Traktor.\n",
      "A man in a neon green and orange uniform is driving on a green tractor.\n",
      "Mehrere Frauen warten in einer Stadt im Freien.\n",
      "Several women wait outside in a city.\n",
      "Eine Frau mit schwarzem Oberteil und Brille streut Puderzucker auf einem Gugelhupf.\n",
      "A lady in a black top with glasses is sprinkling powdered sugar on a bundt cake.\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print(df['german'][i+1])\n",
    "    print(df['english'][i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:55.087685Z",
     "iopub.status.busy": "2024-12-22T13:22:55.087395Z",
     "iopub.status.idle": "2024-12-22T13:22:55.100542Z",
     "shell.execute_reply": "2024-12-22T13:22:55.099704Z",
     "shell.execute_reply.started": "2024-12-22T13:22:55.087665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "num_sentences = len(df)\n",
    "num_train = int(train_ratio * num_sentences)\n",
    "num_val = int(val_ratio * num_sentences)\n",
    "num_test = num_sentences - num_train - num_val\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:55.104138Z",
     "iopub.status.busy": "2024-12-22T13:22:55.103916Z",
     "iopub.status.idle": "2024-12-22T13:22:55.108395Z",
     "shell.execute_reply": "2024-12-22T13:22:55.107528Z",
     "shell.execute_reply.started": "2024-12-22T13:22:55.104120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = df[:num_train]\n",
    "val_df = df[num_train:num_train+num_val]\n",
    "test_df = df_test[:num_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:55.109550Z",
     "iopub.status.busy": "2024-12-22T13:22:55.109316Z",
     "iopub.status.idle": "2024-12-22T13:22:55.118655Z",
     "shell.execute_reply": "2024-12-22T13:22:55.117844Z",
     "shell.execute_reply.started": "2024-12-22T13:22:55.109521Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23197"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:55.119777Z",
     "iopub.status.busy": "2024-12-22T13:22:55.119563Z",
     "iopub.status.idle": "2024-12-22T13:22:55.129824Z",
     "shell.execute_reply": "2024-12-22T13:22:55.129152Z",
     "shell.execute_reply.started": "2024-12-22T13:22:55.119761Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2899"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece model trained for english\n",
      "SentencePiece model trained for german\n",
      "Source (English) vocab size: 7500\n",
      "Target (German) vocab size: 7500\n",
      "Tokenizing training data:\n",
      "Training Data tokenized\n",
      "Tokenizing validation data:\n",
      "Validation Data tokenized\n",
      "Tokenizing testing data:\n",
      "Testing Data tokenized\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Train a SentencePiece tokenizer\n",
    "def train_sentencepiece(corpus, model_prefix, vocab_size=7500):  # Adjust vocab_size\n",
    "    with open(f\"{model_prefix}_corpus.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(corpus))\n",
    "    \n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=f\"{model_prefix}_corpus.txt\",\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        character_coverage=0.9995,  # Adjust for language diversity\n",
    "        pad_id=0, unk_id=1, bos_id=2, eos_id=3\n",
    "    )\n",
    "    print(f\"SentencePiece model trained for {model_prefix}\")\n",
    "\n",
    "\n",
    "# Train tokenizers for English and German\n",
    "train_sentencepiece(train_df['english'], model_prefix=\"english\")\n",
    "train_sentencepiece(train_df['german'], model_prefix=\"german\")\n",
    "\n",
    "# Load SentencePiece models\n",
    "en_tokenizer = spm.SentencePieceProcessor(model_file=\"english.model\")\n",
    "ger_tokenizer = spm.SentencePieceProcessor(model_file=\"german.model\")\n",
    "\n",
    "# Vocabulary sizes\n",
    "src_vocab_size = en_tokenizer.get_piece_size()\n",
    "tgt_vocab_size = ger_tokenizer.get_piece_size()\n",
    "print(f\"Source (English) vocab size: {src_vocab_size}\")\n",
    "print(f\"Target (German) vocab size: {tgt_vocab_size}\")\n",
    "\n",
    "# Tokenizing function with special tokens\n",
    "def tokenize_sentence_with_specials(sentence, tokenizer):\n",
    "    tokens = tokenizer.encode(sentence, out_type=int)  # Convert sentence to subword token IDs\n",
    "    tokens = [tokenizer.bos_id()] + tokens + [tokenizer.eos_id()]  # Add BOS and EOS\n",
    "    return tokens\n",
    "\n",
    "# Tokenizing training data\n",
    "print(\"Tokenizing training data:\")\n",
    "train_en_tokens = [tokenize_sentence_with_specials(sent, en_tokenizer) for sent in train_df['english']]\n",
    "train_gr_tokens = [tokenize_sentence_with_specials(sent, ger_tokenizer) for sent in train_df['german']]\n",
    "print(\"Training Data tokenized\")\n",
    "\n",
    "# Tokenizing validation data\n",
    "print(\"Tokenizing validation data:\")\n",
    "val_en_tokens = [tokenize_sentence_with_specials(sent, en_tokenizer) for sent in val_df['english']]\n",
    "val_gr_tokens = [tokenize_sentence_with_specials(sent, ger_tokenizer) for sent in val_df['german']]\n",
    "print(\"Validation Data tokenized\")\n",
    "\n",
    "# Tokenizing validation data\n",
    "print(\"Tokenizing Test data:\")\n",
    "test_en_tokens = [tokenize_sentence_with_specials(sent, en_tokenizer) for sent in test_df['english']]\n",
    "test_gr_tokens = [tokenize_sentence_with_specials(sent, ger_tokenizer) for sent in test_df['german']]\n",
    "print(\"Test Data tokenized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Dataset and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:55.377498Z",
     "iopub.status.busy": "2024-12-22T13:22:55.377258Z",
     "iopub.status.idle": "2024-12-22T13:22:55.387526Z",
     "shell.execute_reply": "2024-12-22T13:22:55.386848Z",
     "shell.execute_reply.started": "2024-12-22T13:22:55.377481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TranslationDataset(data.Dataset):\n",
    "    def __init__(self, en_tokens, ger_tokens):\n",
    "        self.en_tokens = en_tokens\n",
    "        self.ger_tokens = ger_tokens\n",
    "        self.max_len = max(max(len(en), len(gr)) for en, gr in zip(en_tokens, ger_tokens))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.en_tokens)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        en_data = self.en_tokens[index] + [0] * (self.max_len - len(self.en_tokens[index]))  # Padding with 0\n",
    "        ger_data = self.ger_tokens[index] + [0] * (self.max_len - len(self.ger_tokens[index]))  # Padding with 0\n",
    "        return torch.tensor(en_data), torch.tensor(ger_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:55.388834Z",
     "iopub.status.busy": "2024-12-22T13:22:55.388502Z",
     "iopub.status.idle": "2024-12-22T13:22:55.410270Z",
     "shell.execute_reply": "2024-12-22T13:22:55.409579Z",
     "shell.execute_reply.started": "2024-12-22T13:22:55.388790Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and Dataloders created\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TranslationDataset(train_en_tokens, train_gr_tokens)\n",
    "val_dataset = TranslationDataset(val_en_tokens, val_gr_tokens)\n",
    "test_dataset=TranslationDataset(test_en_tokens,test_gr_tokens)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=2)\n",
    "test_loader=data.DataLoader(test_dataset,batch_size=2)\n",
    "print(\"Dataset and Dataloders created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:55.411416Z",
     "iopub.status.busy": "2024-12-22T13:22:55.411195Z",
     "iopub.status.idle": "2024-12-22T13:22:55.473132Z",
     "shell.execute_reply": "2024-12-22T13:22:55.472365Z",
     "shell.execute_reply.started": "2024-12-22T13:22:55.411399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Data (Batch 0): tensor([[   2,    6,   19,   25,    4,  220,   90,  104,   79,   21,    4,  212,\n",
      "            5,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   2,    6,  172,  129,   16,    4,  214,  119,  507,  379,  180,   15,\n",
      "           80,   60, 1268,    5,    3,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]])\n",
      "German Data (Batch 0): tensor([[   2,   19,   22,    9, 1156,   30,  114,   40,   68,    6,    7,  241,\n",
      "            5,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [   2,    8,  217,   46,    1,   50,    6,  334,   11,   13,   35,   40,\n",
      "            1,   24,    1,  828,   23, 1345,  624, 2806, 7318, 3160,   24, 7441,\n",
      "            1,    5,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "# Print the first batch of data from train_loader\n",
    "for batch_idx, (en_data, ger_data) in enumerate(train_loader):\n",
    "    print(\"English Data (Batch 0):\", en_data)\n",
    "    print(\"German Data (Batch 0):\", ger_data)\n",
    "    break  # Break after printing the first batch to avoid printing the entire dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:55.481268Z",
     "iopub.status.busy": "2024-12-22T13:22:55.481022Z",
     "iopub.status.idle": "2024-12-22T13:22:55.487131Z",
     "shell.execute_reply": "2024-12-22T13:22:55.486422Z",
     "shell.execute_reply.started": "2024-12-22T13:22:55.481250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:55.488153Z",
     "iopub.status.busy": "2024-12-22T13:22:55.487932Z",
     "iopub.status.idle": "2024-12-22T13:22:55.494924Z",
     "shell.execute_reply": "2024-12-22T13:22:55.494180Z",
     "shell.execute_reply.started": "2024-12-22T13:22:55.488136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        # Frequency Scaling mechanism \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:55.506674Z",
     "iopub.status.busy": "2024-12-22T13:22:55.506395Z",
     "iopub.status.idle": "2024-12-22T13:22:55.515836Z",
     "shell.execute_reply": "2024-12-22T13:22:55.514997Z",
     "shell.execute_reply.started": "2024-12-22T13:22:55.506648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, debug_str=None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.debug_str = debug_str\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "\n",
    "        \n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "         \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(Q.size(0), -1, self.d_model)\n",
    "        output = self.W_o(attn_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:55.528585Z",
     "iopub.status.busy": "2024-12-22T13:22:55.528306Z",
     "iopub.status.idle": "2024-12-22T13:22:55.534219Z",
     "shell.execute_reply": "2024-12-22T13:22:55.533442Z",
     "shell.execute_reply.started": "2024-12-22T13:22:55.528560Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:55.552414Z",
     "iopub.status.busy": "2024-12-22T13:22:55.551711Z",
     "iopub.status.idle": "2024-12-22T13:22:55.557924Z",
     "shell.execute_reply": "2024-12-22T13:22:55.557290Z",
     "shell.execute_reply.started": "2024-12-22T13:22:55.552389Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)#, debug_str=\"cross\")\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        \n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout, pad_token_src=0, pad_token_tgt=0, device='cpu'):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.pad_token_src = pad_token_src\n",
    "        self.pad_token_tgt = pad_token_tgt\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "        \n",
    "        # Define the encoder and decoder layers\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "    def generate_mask(self, src_mask, tgt_mask):\n",
    "     # Ensure the src_mask is correctly shaped (batch_size, 1, 1, seq_len)\n",
    "       src_mask = src_mask.unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    # Ensure the tgt_mask is correctly shaped (batch_size, 1, seq_len, seq_len)\n",
    "       tgt_mask = tgt_mask.unsqueeze(1).unsqueeze(3)\n",
    "    \n",
    "    # Sequence length for nopeak_mask\n",
    "       seq_length = tgt_mask.size(2)\n",
    "    \n",
    "    # Generate lower triangular no-peak mask (1 for allowed, 0 for masked)\n",
    "       nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "\n",
    "    # Apply the no-peak mask to the target mask\n",
    "       tgt_mask = tgt_mask & nopeak_mask.to(self.device)\n",
    "\n",
    "       return src_mask, tgt_mask\n",
    "\n",
    "    \n",
    "    def decode(self, src, bos_token_id, eos_token_id, mask=None, max_dec_length=50):\n",
    "     # Initialize the target tensor with the BOS token\n",
    "      tgt = torch.tensor([[bos_token_id]] * src.shape[0]).to(self.device)\n",
    "\n",
    "    # Initialize src_mask and tgt_mask if mask is provided, else generate them\n",
    "      if mask:\n",
    "         src_mask = mask.get('src_mask')\n",
    "         tgt_mask = mask.get('tgt_mask', torch.ones((tgt.size(0), tgt.size(1), tgt.size(1)), device=self.device).bool())\n",
    "      else:\n",
    "        # Generate default masks if no mask is provided\n",
    "          src_mask = src != self.pad_token_src\n",
    "          tgt_mask = torch.ones((tgt.size(0), tgt.size(1), tgt.size(1)), device=self.device).bool()\n",
    "\n",
    "    # Now apply the masks (src_mask and tgt_mask) to the model\n",
    "      src_mask, tgt_mask = self.generate_mask(src_mask, tgt_mask)\n",
    "\n",
    "      for _ in range(max_dec_length):\n",
    "        # Forward pass through the model with the masks applied\n",
    "          logits = self.forward(src, tgt, {'src_mask': src_mask, 'tgt_mask': tgt_mask})\n",
    "          next_token = logits.argmax(-1)[:, -1].unsqueeze(1)\n",
    "\n",
    "        # Add the predicted token to the target sequence\n",
    "          tgt = torch.cat([tgt, next_token], dim=1)\n",
    "\n",
    "        # Break if the EOS token is generated\n",
    "          if torch.any(next_token == eos_token_id):\n",
    "             break\n",
    "\n",
    "          return tgt\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, mask=None):\n",
    "        if mask:\n",
    "            src_mask, tgt_mask = self.generate_mask(mask['src_mask'], mask['tgt_mask'])\n",
    "        else:\n",
    "            src_mask, tgt_mask = self.generate_mask(src != self.pad_token_src, tgt != self.pad_token_tgt)\n",
    "        \n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:55.595902Z",
     "iopub.status.busy": "2024-12-22T13:22:55.595655Z",
     "iopub.status.idle": "2024-12-22T13:22:55.945174Z",
     "shell.execute_reply": "2024-12-22T13:22:55.944480Z",
     "shell.execute_reply.started": "2024-12-22T13:22:55.595884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#Hyperparameters\n",
    "# Reduce Model Size\n",
    "d_model = 64  # Decrease the model dimensionality\n",
    "num_heads = 2  # Decrease the number of attention heads\n",
    "num_layers = 2  # Decrease the number of layers\n",
    "d_ff = 512  # Decrease the size of the feed-forward layers\n",
    "max_seq_length = max(train_dataset.max_len, val_dataset.max_len, test_dataset.max_len)  # Maximum sequence length\n",
    "dropout = 0.1  # Dropout probability\n",
    "num_epochs=10  # Number of epochs\n",
    "# Instantiate the Transformer model\n",
    "transformer_model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:55.952606Z",
     "iopub.status.busy": "2024-12-22T13:22:55.952375Z",
     "iopub.status.idle": "2024-12-22T13:22:57.637425Z",
     "shell.execute_reply": "2024-12-22T13:22:57.636776Z",
     "shell.execute_reply.started": "2024-12-22T13:22:55.952579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(transformer_model.parameters(),lr=0.001)\n",
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:57.639346Z",
     "iopub.status.busy": "2024-12-22T13:22:57.638499Z",
     "iopub.status.idle": "2024-12-22T13:22:57.643887Z",
     "shell.execute_reply": "2024-12-22T13:22:57.643066Z",
     "shell.execute_reply.started": "2024-12-22T13:22:57.639315Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder_embedding): Embedding(7500, 64)\n",
      "  (decoder_embedding): Embedding(7500, 64)\n",
      "  (positional_encoding): PositionalEncoding()\n",
      "  (fc): Linear(in_features=64, out_features=7500, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-1): 2 x EncoderLayer(\n",
      "      (self_attn): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (W_k): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (W_v): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (W_o): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (feed_forward): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-1): 2 x DecoderLayer(\n",
      "      (self_attn): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (W_k): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (W_v): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (W_o): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (cross_attn): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (W_k): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (W_v): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (W_o): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (feed_forward): PositionWiseFeedForward(\n",
      "        (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T13:22:57.645756Z",
     "iopub.status.busy": "2024-12-22T13:22:57.644946Z",
     "iopub.status.idle": "2024-12-22T13:22:57.653304Z",
     "shell.execute_reply": "2024-12-22T13:22:57.652593Z",
     "shell.execute_reply.started": "2024-12-22T13:22:57.645730Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 1/11599 [00:00<10:41, 18.07batch/s, Loss=8.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 1/1450 [00:00<00:21, 68.91batch/s, Validation Loss=7.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0048\n",
      "Model saved at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|          | 1/11599 [00:00<07:33, 25.58batch/s, Loss=6.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Average Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 1/1450 [00:00<00:16, 87.95batch/s, Validation Loss=6.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0042\n",
      "Model saved at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|          | 1/11599 [00:00<08:58, 21.54batch/s, Loss=6.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Average Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 1/1450 [00:00<00:14, 101.02batch/s, Validation Loss=5.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0039\n",
      "Model saved at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|          | 1/11599 [00:00<08:22, 23.10batch/s, Loss=5.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Average Loss: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 1/1450 [00:00<00:14, 97.60batch/s, Validation Loss=5.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0038\n",
      "Model saved at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   0%|          | 1/11599 [00:00<07:32, 25.66batch/s, Loss=5.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Average Loss: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 1/1450 [00:00<00:15, 93.49batch/s, Validation Loss=5.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0036\n",
      "Model saved at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   0%|          | 1/11599 [00:00<07:35, 25.45batch/s, Loss=5.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Average Loss: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 1/1450 [00:00<00:15, 90.98batch/s, Validation Loss=5.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0036\n",
      "Model saved at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|          | 1/11599 [00:00<05:32, 34.83batch/s, Loss=5.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Average Loss: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 1/1450 [00:00<00:16, 88.66batch/s, Validation Loss=5.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0035\n",
      "Model saved at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   0%|          | 1/11599 [00:00<07:19, 26.37batch/s, Loss=5.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Average Loss: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 1/1450 [00:00<00:21, 68.60batch/s, Validation Loss=4.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0034\n",
      "Model saved at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   0%|          | 1/11599 [00:00<06:18, 30.65batch/s, Loss=6.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Average Loss: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 1/1450 [00:00<00:27, 52.64batch/s, Validation Loss=4.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0034\n",
      "Model saved at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   0%|          | 1/11599 [00:00<07:13, 26.75batch/s, Loss=5.12]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Average Loss: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 1/1450 [00:00<00:06, 211.73batch/s, Validation Loss=4.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0033\n",
      "Model saved at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 3  # Number of epochs to wait for improvement before stopping\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    transformer_model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Create a progress bar\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "    \n",
    "    # Iterate through batches\n",
    "    for batch_idx, (src, tgt) in progress_bar:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = transformer_model(src, tgt[:, :-1])  # Exclude the <eos> token from input      \n",
    "        # Flatten the output and target tensors to compute loss\n",
    "        output_flat = output.view(-1, output.size(-1))\n",
    "        tgt_flat = tgt[:, 1:].contiguous().view(-1)  # Exclude the <bos> token from target\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output_flat, tgt_flat)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), max_norm=1)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Add batch loss to total loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar description\n",
    "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "    \n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Validation  transformer_model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Create a progress bar for validation\n",
    "        val_progress_bar = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validation\", unit=\"batch\")\n",
    "        \n",
    "        for batch_idx, (src, tgt) in val_progress_bar:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = transformer_model(src, tgt[:, :-1])  # Exclude the <eos> token from input\n",
    "            \n",
    "            # Flatten the output and target tensors to compute loss\n",
    "            output_flat = output.view(-1, output.size(-1))  \n",
    "            tgt_flat = tgt[:, 1:].contiguous().view(-1)  # Exclude the <bos> token from target\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output_flat, tgt_flat)\n",
    "            \n",
    "            # Add batch loss to total loss\n",
    "            val_loss += loss.item()\n",
    "             \n",
    "            # Update progress bar description\n",
    "            val_progress_bar.set_postfix({\"Validation Loss\": loss.item()})\n",
    "\n",
    "    # Calculate average validation loss\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping logic\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_without_improvement = 0  # Reset the counter\n",
    "        # Save the model\n",
    "        torch.save(transformer_model.state_dict(), 'transformer_model.pth')\n",
    "        print(f\"Model saved at epoch {epoch+1}\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    # Check for early stopping\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}. Validation loss did not improve.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 11599/11599 [07:03<00:00, 27.38batch/s, Loss=0.881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Loss: 1.0510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1450/1450 [00:14<00:00, 103.38batch/s, Validation Loss=0.906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9542\n",
      "Validation ROUGE Scores: {'rouge1': 0.43283835715091595, 'rouge2': 0.19386879035341922, 'rougeL': 0.4162730873912906}\n",
      "Model saved at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 11599/11599 [07:20<00:00, 26.34batch/s, Loss=0.701] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Average Loss: 0.8318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1450/1450 [00:13<00:00, 105.22batch/s, Validation Loss=0.785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8407\n",
      "Validation ROUGE Scores: {'rouge1': 0.5023131110557396, 'rouge2': 0.2570274930347473, 'rougeL': 0.4851970226480362}\n",
      "Model saved at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 11599/11599 [07:20<00:00, 26.32batch/s, Loss=0.802] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Average Loss: 0.7503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1450/1450 [00:14<00:00, 100.94batch/s, Validation Loss=0.729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7880\n",
      "Validation ROUGE Scores: {'rouge1': 0.5347638125299945, 'rouge2': 0.28935758130210404, 'rougeL': 0.5160682260691485}\n",
      "Model saved at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 11599/11599 [07:26<00:00, 25.99batch/s, Loss=0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Average Loss: 0.6993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1450/1450 [00:14<00:00, 101.25batch/s, Validation Loss=0.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7479\n",
      "Validation ROUGE Scores: {'rouge1': 0.5607894043232357, 'rouge2': 0.32053016506502635, 'rougeL': 0.5438058675530372}\n",
      "Model saved at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 11599/11599 [07:39<00:00, 25.23batch/s, Loss=0.249] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Average Loss: 0.6625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1450/1450 [00:13<00:00, 107.01batch/s, Validation Loss=0.637] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7135\n",
      "Validation ROUGE Scores: {'rouge1': 0.5769357531802678, 'rouge2': 0.33625503623856834, 'rougeL': 0.559124122756387}\n",
      "Model saved at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 11599/11599 [07:46<00:00, 24.86batch/s, Loss=1.1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Average Loss: 0.6332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1450/1450 [00:14<00:00, 102.57batch/s, Validation Loss=0.759] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6885\n",
      "Validation ROUGE Scores: {'rouge1': 0.5894155829211623, 'rouge2': 0.3569996679065852, 'rougeL': 0.5734363165611484}\n",
      "Model saved at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 11599/11599 [07:35<00:00, 25.47batch/s, Loss=0.897] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Average Loss: 0.6103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1450/1450 [00:14<00:00, 97.45batch/s, Validation Loss=0.673]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6725\n",
      "Validation ROUGE Scores: {'rouge1': 0.5962022664634847, 'rouge2': 0.362150133390866, 'rougeL': 0.5807996008775139}\n",
      "Model saved at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 11599/11599 [21:46<00:00,  8.88batch/s, Loss=0.606]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Average Loss: 0.5922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1450/1450 [00:14<00:00, 101.19batch/s, Validation Loss=0.643] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6596\n",
      "Validation ROUGE Scores: {'rouge1': 0.605010966631962, 'rouge2': 0.3746338651756154, 'rougeL': 0.5899880129688385}\n",
      "Model saved at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 11599/11599 [09:41<00:00, 19.94batch/s, Loss=0.724]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Average Loss: 0.5757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1450/1450 [00:14<00:00, 102.56batch/s, Validation Loss=0.695] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6558\n",
      "Validation ROUGE Scores: {'rouge1': 0.6099110882217844, 'rouge2': 0.3763702754133648, 'rougeL': 0.5932270568663334}\n",
      "Model saved at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 11599/11599 [11:07<00:00, 17.38batch/s, Loss=0.487]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Average Loss: 0.5644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1450/1450 [00:14<00:00, 103.08batch/s, Validation Loss=0.624] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6454\n",
      "Validation ROUGE Scores: {'rouge1': 0.6123742827673571, 'rouge2': 0.3820343653436769, 'rougeL': 0.5971016769158085}\n",
      "Model saved at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 3  # Number of epochs to wait for improvement before stopping\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "\n",
    "# Create a ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    transformer_model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Create a progress bar for training\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "    \n",
    "    # Iterate through batches for training\n",
    "    for batch_idx, (src, tgt) in progress_bar:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = transformer_model(src, tgt[:, :-1])  # Exclude the <eos> token from input\n",
    "        # Flatten the output and target tensors to compute loss\n",
    "        output_flat = output.view(-1, output.size(-1))\n",
    "        tgt_flat = tgt[:, 1:].contiguous().view(-1)  # Exclude the <bos> token from target\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output_flat, tgt_flat)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), max_norm=1)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Add batch loss to total loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar description\n",
    "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "    \n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    transformer_model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0\n",
    "    val_rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Create a progress bar for validation\n",
    "        val_progress_bar = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validation\", unit=\"batch\")\n",
    "        \n",
    "        for batch_idx, (src, tgt) in val_progress_bar:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = transformer_model(src, tgt[:, :-1])  # Exclude the <eos> token from input\n",
    "            \n",
    "            # Flatten the output and target tensors to compute loss\n",
    "            output_flat = output.view(-1, output.size(-1))  \n",
    "            tgt_flat = tgt[:, 1:].contiguous().view(-1)  # Exclude the <bos> token from target\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output_flat, tgt_flat)\n",
    "            \n",
    "            # Add batch loss to total validation loss\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Decode the predictions to text (argmax over the vocab dimension)\n",
    "            pred_tokens = output.argmax(dim=-1)  # Taking the predicted tokens (removes vocab dimension)\n",
    "            \n",
    "            # Loop through each pair of predicted and true targets for ROUGE calculation\n",
    "            for pred, true in zip(pred_tokens, tgt):\n",
    "                pred_text = ger_tokenizer.decode(pred.tolist())\n",
    "                true_text = ger_tokenizer.decode(true.tolist())\n",
    "                \n",
    "                # Compute ROUGE score for each pair and append the F-measure values\n",
    "                score = scorer.score(true_text, pred_text)\n",
    "                for key in val_rouge_scores:\n",
    "                    val_rouge_scores[key].append(score[key].fmeasure)\n",
    "\n",
    "            # Update progress bar description\n",
    "            val_progress_bar.set_postfix({\"Validation Loss\": loss.item()})\n",
    "\n",
    "    # Calculate average validation loss\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Calculate the average ROUGE scores\n",
    "    avg_rouge_scores = {key: sum(values) / len(values) for key, values in val_rouge_scores.items()}\n",
    "    print(f\"Validation ROUGE Scores: {avg_rouge_scores}\")\n",
    "    \n",
    "    # Early stopping logic\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_without_improvement = 0  # Reset the counter\n",
    "        # Save the model\n",
    "        torch.save(transformer_model.state_dict(), 'transformer_model.pth')\n",
    "        print(f\"Model saved at epoch {epoch+1}\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    # Check for early stopping\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}. Validation loss did not improve.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 500/500 [00:04<00:00, 108.20batch/s, Test Loss=0.216]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7769\n",
      "Test ROUGE Scores: {'rouge1': 0.6174154518047238, 'rouge2': 0.38755095282171126, 'rougeL': 0.6033373757267394}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "transformer_model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0\n",
    "test_rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "# Create a ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "with torch.no_grad():\n",
    "        # Create a progress bar for validation\n",
    "        test_progress_bar = tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Test\", unit=\"batch\")\n",
    "        \n",
    "        for batch_idx, (src, tgt) in test_progress_bar:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = transformer_model(src, tgt[:, :-1])  # Exclude the <eos> token from input\n",
    "            \n",
    "            # Flatten the output and target tensors to compute loss\n",
    "            output_flat = output.view(-1, output.size(-1))  \n",
    "            tgt_flat = tgt[:, 1:].contiguous().view(-1)  # Exclude the <bos> token from target\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output_flat, tgt_flat)\n",
    "            \n",
    "            # Add batch loss to total validation loss\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Decode the predictions to text (argmax over the vocab dimension)\n",
    "            pred_tokens = output.argmax(dim=-1)  # Taking the predicted tokens (removes vocab dimension)\n",
    "            \n",
    "            # Loop through each pair of predicted and true targets for ROUGE calculation\n",
    "            for pred, true in zip(pred_tokens, tgt):\n",
    "                pred_text = ger_tokenizer.decode(pred.tolist())\n",
    "                true_text = ger_tokenizer.decode(true.tolist())\n",
    "                \n",
    "                # Compute ROUGE score for each pair and append the F-measure values\n",
    "                score = scorer.score(true_text, pred_text)\n",
    "                for key in test_rouge_scores:\n",
    "                    test_rouge_scores[key].append(score[key].fmeasure)\n",
    "\n",
    "            # Update progress bar description\n",
    "            test_progress_bar.set_postfix({\"Test Loss\": loss.item()})\n",
    "\n",
    "    # Calculate average validation loss\n",
    "avg_loss = test_loss / len(test_loader)\n",
    "print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Calculate the average ROUGE scores\n",
    "avg_rouge_scores = {key: sum(values) / len(values) for key, values in test_rouge_scores\n",
    "                    .items()}\n",
    "print(f\"Test ROUGE Scores: {avg_rouge_scores}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1118439,
     "sourceId": 1878727,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4937140,
     "sourceId": 8311007,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6352671,
     "sourceId": 10268057,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
